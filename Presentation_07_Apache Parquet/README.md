# Apache Parquet : Format de Stockage Colonnaire Haute Performance pour l'Analyse Big Data

Apache Parquet est un format de stockage colonnaire open-source con√ßu pour optimiser les performances d'analyse et r√©duire les co√ªts de stockage dans l'√©cosyst√®me Big Data.

## üìå Objectifs

La pr√©sentation vise √† √©clairer les points suivants concernant Apache Parquet :

**Optimiser les performances d'analyse BI** : Apache Parquet am√©liore drastiquement les performances des tableaux de bord et rapports Business Intelligence gr√¢ce √† son format colonnaire qui permet de lire uniquement les colonnes n√©cessaires pour les KPI, r√©duisant ainsi les temps de g√©n√©ration des rapports.

**R√©duire les co√ªts de stockage des donn√©es d'entreprise** : Gr√¢ce √† ses algorithmes de compression avanc√©s, Parquet peut r√©duire la taille des fichiers de donn√©es m√©tier de 75% ou plus par rapport aux formats traditionnels comme CSV ou Excel, optimisant ainsi les co√ªts de stockage des entrep√¥ts de donn√©es.

**Faciliter l'int√©gration avec les outils BI** : Parquet est nativement support√© par la plupart des outils BI modernes (Power BI, Tableau, Qlik, etc.) et les plateformes d'analyse (Pandas, R), permettant une int√©gration transparente dans les pipelines de reporting existants.

**Pr√©senter l'architecture orient√©e BI** : Comprendre comment la structure colonnaire de Parquet optimise les requ√™tes d'agr√©gation typiques en BI (SUM, COUNT, AVG) et am√©liore les performances des filtres et des drill-down.

**Illustrer des cas d'utilisation BI concrets** : D√©montrer comment Apache Parquet peut √™tre appliqu√© pour l'analyse de donn√©es de taxis dans un contexte BI, l'optimisation des rapports sur de gros volumes de donn√©es transactionnelles, et l'am√©lioration des performances des dashboards interactifs.

## üõ†Ô∏è Pr√©requis

### Techniques
- Python (connaissances interm√©diaires)
- Pandas et PyArrow
- Jupyter Notebook
- Notions de formats de donn√©es
- Git pour la gestion de version

### Connaissances
- Notions de base en Business Intelligence et reporting
- Concepts fondamentaux des entrep√¥ts de donn√©es (Data Warehouse)
- Compr√©hension des KPI et m√©triques m√©tier

## üöÄ Instructions d'utilisation

### 1. Setup de l'environnement

**Option A : Environnement local (recommand√©)**

```bash
# Cloner le repository
git clone https://github.com/hamzaaouni/BI_Presentations.git
cd Presentation_07_Apache Parquet/Demo

# Cr√©er un environnement virtuel
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate     # Windows

# Installer les d√©pendances
pip install pandas pyarrow jupyter matplotlib seaborn
```

**Option B : Google Colab**
- Ouvrir Google Colab
- Uploader le fichier Demo.ipynb
- Installer les d√©pendances dans la premi√®re cellule

### 2. Pr√©parer les donn√©es

```python
# V√©rifier la pr√©sence du dataset
import pandas as pd
import pyarrow.parquet as pq

# Charger le dataset taxi_data
df = pd.read_csv('taxi_data.csv')  # ou le format original
print(f"Dataset shape: {df.shape}")
print(df.head())
```

### 3. Conversion et analyse

```python
# Convertir en format Parquet
df.to_parquet('taxi_data.parquet', engine='pyarrow')

# Comparer les tailles de fichiers
import os
csv_size = os.path.getsize('taxi_data.csv')
parquet_size = os.path.getsize('taxi_data.parquet')
compression_ratio = (csv_size - parquet_size) / csv_size * 100

print(f"Taille CSV: {csv_size/1024/1024:.2f} MB")
print(f"Taille Parquet: {parquet_size/1024/1024:.2f} MB")
print(f"Compression: {compression_ratio:.1f}%")
```

### 4. Ex√©cuter la d√©monstration

```bash
# Lancer Jupyter Notebook
jupyter notebook Demo.ipynb
```

### 5. R√©sultats attendus

- R√©duction significative de la taille des fichiers (60-80% typiquement)
- Am√©lioration des performances de lecture pour les requ√™tes analytiques
- D√©monstration des capacit√©s de filtrage et d'agr√©gation optimis√©es

## üìä Contenu

- **Demo.ipynb** : Notebook Jupyter complet avec toutes les d√©monstrations pratiques
- **taxi_data** : Dataset utilis√© pour illustrer les avantages de Parquet
- **Slides** : Pr√©sentation th√©orique expliquant les concepts et l'architecture
- **Scripts** : Utilitaires pour la conversion et l'analyse comparative

## üîó Ressources compl√©mentaires

- üìπ [Documentation Apache Parquet](https://parquet.apache.org/docs/)
- üìö [Guide PyArrow](https://arrow.apache.org/docs/python/)
- üéì [Tutoriels Pandas avec Parquet](https://pandas.pydata.org/docs/user_guide/io.html#parquet)
- üõ†Ô∏è [Apache Arrow](https://arrow.apache.org/)

## üìà M√©triques de Performance BI

Le projet d√©montre les am√©liorations suivantes pour les use cases Business Intelligence :

- **Compression des donn√©es m√©tier** : R√©duction de 60-80% de la taille des fichiers de donn√©es transactionnelles
- **Performance des rapports** : Acc√©l√©ration de 3-10x pour la g√©n√©ration de tableaux de bord
- **Requ√™tes sur colonnes sp√©cifiques** : Lecture jusqu'√† 100x plus rapide pour les KPI calcul√©s sur quelques m√©triques
- **Filtres et drill-down** : Optimisation des filtres temporels et g√©ographiques pour l'analyse interactive

## üîß Cas d'usage BI d√©montr√©s

1. **Rapports de performance temporelle** : G√©n√©ration rapide de rapports sur les donn√©es de trajets par p√©riode (journalier, mensuel, trimestriel)
2. **Tableaux de bord g√©ographiques** : Analyse des KPI de d√©placement par zone g√©ographique avec drill-down optimis√©
3. **Optimisation des requ√™tes BI** : Comparaison des temps de g√©n√©ration de rapports CSV vs Parquet
4. **Int√©gration avec les outils BI** : Connexion directe avec Power BI, Tableau, et outils de reporting

## üë• Auteur(s)

**Nom** : Anas Roukhmi & Abdlkrim Rekbi  
**Promotion** : Master Big Data & IA ‚Äì 2024/2025  
**Contact** : [roukhmi02@gmail.com]

---

*Ce projet fait partie du cursus Master Big Data & IA et vise √† d√©montrer l'importance des formats de stockage optimis√©s dans les architectures Big Data modernes.*